{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using the New York Times API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using API tool with key\n",
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('51ae5c44eb962681341060ede81808b8:11:73610715')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the dates to use for sentiment analysis\n",
    "dframe=pd.read_csv('data/IYZ.csv')\n",
    "date_list = list(dframe['date'])\n",
    "cleaned_dates = []\n",
    "for entry in date_list:\n",
    "    cleaned_dates.append(entry.replace('-', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes in a response to the NYT api and parses the articles into a list of dictionaries\n",
    "def parse_articles(articles, datestamp):  \n",
    "    news = []\n",
    "    for i in articles['response']['docs']:\n",
    "        dic = {}\n",
    "        dic['date'] = datestamp\n",
    "        dic['text'] = i['headline']['main'].encode(\"utf8\")\n",
    "        if i['snippet'] is not None:\n",
    "            dic['text'] = dic['text'] + \" \" + i['snippet'].encode(\"utf8\")     \n",
    "        news.append(dic)\n",
    "    return(news) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function accepts a list of dates and returns a dictionary of parsed articles for those dates \n",
    "def get_articles(dates,query):\n",
    "    all_articles = []\n",
    "    for date in dates:\n",
    "        articles = api.search(q = query,\n",
    "                fq = {'news_desk':['Business','Financial', 'Jobs','Retail','Outlook',\n",
    "                                   'Personal Investing','Technology','Wealth']},\n",
    "                begin_date = date,\n",
    "                end_date = date,\n",
    "                sort='oldest')\n",
    "        articles = parse_articles(articles,date)\n",
    "        if len(articles) != 0:\n",
    "            all_articles  = all_articles + articles\n",
    "        time.sleep(0.1)\n",
    "    return(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verizon_articles\n",
    "verizon_articles = get_articles(cleaned_dates,'Verizon')\n",
    "verizon_df = pd.DataFrame.from_dict(verizon_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store dataframe in a CSV for future analysis\n",
    "verizon_df.to_csv('data/verizon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the CSV after manual classification of text as positive or negative\n",
    "verizon_df = pd.read_csv('data/verizon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert this dataframe back into a dictionary for faster processing\n",
    "clean_dict = verizon_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize and clean the text\n",
    "def make_sentence(word_arr):\n",
    "    temp_sentence = \"\"\n",
    "    for word in word_arr:\n",
    "        temp_sentence = temp_sentence + word + \" \"\n",
    "    return temp_sentence\n",
    "\n",
    "text_arr = clean_dict['text'].values()\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "clean_arr = []\n",
    "\n",
    "for sentence in text_arr:\n",
    "    words = sentence.split()\n",
    "    clean_sentence = []\n",
    "    for word in words:\n",
    "        if len(word) < 4:\n",
    "            words.remove(word)\n",
    "        clean_word = regex.sub('', word)\n",
    "        clean_sentence.append(clean_word)\n",
    "    clean_arr.append(make_sentence(clean_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "839"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
