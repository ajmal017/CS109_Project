{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are We Rich Yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%run talibref.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data: Ticker(IYZ) from 2000.05.26 to 2015.11.27\n",
      "Usable data: Ticker(IYZ) from 2001.03.14 to 2015.11.27 \n",
      "Returned data: Ticker(IYZ) from 2010.01.04 to 2015.11.27 \n",
      "Save path: data/IYZ_from_2010.01.04_2015.11.27.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_1</th>\n",
       "      <th>result_1</th>\n",
       "      <th>perf_1</th>\n",
       "      <th>close_14</th>\n",
       "      <th>result_14</th>\n",
       "      <th>perf_14</th>\n",
       "      <th>results</th>\n",
       "      <th>bb_upper</th>\n",
       "      <th>bb_middle</th>\n",
       "      <th>bb_lower</th>\n",
       "      <th>bb_pct</th>\n",
       "      <th>bb_bandwidth</th>\n",
       "      <th>bb_squeeze</th>\n",
       "      <th>bb_signalup</th>\n",
       "      <th>bb_signaldn</th>\n",
       "      <th>bb_signal</th>\n",
       "      <th>ema50</th>\n",
       "      <th>ema150</th>\n",
       "      <th>ema200</th>\n",
       "      <th>ema_signal1</th>\n",
       "      <th>ema_signal2</th>\n",
       "      <th>kama50</th>\n",
       "      <th>kama150</th>\n",
       "      <th>kama200</th>\n",
       "      <th>kama_signal1</th>\n",
       "      <th>kama_signal2</th>\n",
       "      <th>sar</th>\n",
       "      <th>sar_signal</th>\n",
       "      <th>adx</th>\n",
       "      <th>plus_di</th>\n",
       "      <th>minus_di</th>\n",
       "      <th>adx_trend</th>\n",
       "      <th>adx_direction</th>\n",
       "      <th>adx_signal</th>\n",
       "      <th>aroon_osc</th>\n",
       "      <th>aroon_signal</th>\n",
       "      <th>cci</th>\n",
       "      <th>cci_signal</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_sigline</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>ppo</th>\n",
       "      <th>ppo_signal</th>\n",
       "      <th>mfi</th>\n",
       "      <th>mfi_signal</th>\n",
       "      <th>roc</th>\n",
       "      <th>roc_signal</th>\n",
       "      <th>rsi</th>\n",
       "      <th>rsi_signal</th>\n",
       "      <th>ult_osc</th>\n",
       "      <th>ult_signal</th>\n",
       "      <th>willr</th>\n",
       "      <th>wr_signal</th>\n",
       "      <th>ad_osc</th>\n",
       "      <th>ad_signal</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>sslow_signal</th>\n",
       "      <th>stoch_fastk</th>\n",
       "      <th>stoch_fastd</th>\n",
       "      <th>srsi_signal</th>\n",
       "      <th>trix</th>\n",
       "      <th>trix_signal</th>\n",
       "      <th>sr_pivotpts</th>\n",
       "      <th>sr_res1</th>\n",
       "      <th>sr_sup1</th>\n",
       "      <th>sr_res2</th>\n",
       "      <th>sr_sup2</th>\n",
       "      <th>sr_res3</th>\n",
       "      <th>sr_sup3</th>\n",
       "      <th>cv_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>20.370001</td>\n",
       "      <td>20.549999</td>\n",
       "      <td>20.270000</td>\n",
       "      <td>20.549999</td>\n",
       "      <td>664200</td>\n",
       "      <td>20.680000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>18.719999</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.089051</td>\n",
       "      <td>0</td>\n",
       "      <td>20.545079</td>\n",
       "      <td>19.9480</td>\n",
       "      <td>19.350920</td>\n",
       "      <td>1.004120</td>\n",
       "      <td>5.986360</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>19.245605</td>\n",
       "      <td>18.471530</td>\n",
       "      <td>18.441038</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.030765</td>\n",
       "      <td>18.366468</td>\n",
       "      <td>18.470379</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.822958</td>\n",
       "      <td>1</td>\n",
       "      <td>34.171492</td>\n",
       "      <td>33.666344</td>\n",
       "      <td>15.692957</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>122.917840</td>\n",
       "      <td>1</td>\n",
       "      <td>0.393756</td>\n",
       "      <td>0.403865</td>\n",
       "      <td>-0.010110</td>\n",
       "      <td>0</td>\n",
       "      <td>2.048084</td>\n",
       "      <td>1</td>\n",
       "      <td>79.657038</td>\n",
       "      <td>0</td>\n",
       "      <td>4.900454</td>\n",
       "      <td>0</td>\n",
       "      <td>68.259660</td>\n",
       "      <td>0</td>\n",
       "      <td>56.239960</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>694259.549529</td>\n",
       "      <td>1</td>\n",
       "      <td>49.166583</td>\n",
       "      <td>52.670801</td>\n",
       "      <td>0</td>\n",
       "      <td>81.338670</td>\n",
       "      <td>27.112890</td>\n",
       "      <td>0</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>1</td>\n",
       "      <td>20.456666</td>\n",
       "      <td>22.773333</td>\n",
       "      <td>20.363333</td>\n",
       "      <td>22.866666</td>\n",
       "      <td>18.046666</td>\n",
       "      <td>25.183333</td>\n",
       "      <td>17.953333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>20.610001</td>\n",
       "      <td>20.760000</td>\n",
       "      <td>20.459999</td>\n",
       "      <td>20.680000</td>\n",
       "      <td>1214800</td>\n",
       "      <td>20.340000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.016441</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.086074</td>\n",
       "      <td>0</td>\n",
       "      <td>20.632786</td>\n",
       "      <td>20.0125</td>\n",
       "      <td>19.392214</td>\n",
       "      <td>1.038058</td>\n",
       "      <td>6.198986</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>19.301856</td>\n",
       "      <td>18.500781</td>\n",
       "      <td>18.463316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.120883</td>\n",
       "      <td>18.404560</td>\n",
       "      <td>18.516717</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.881121</td>\n",
       "      <td>1</td>\n",
       "      <td>34.807420</td>\n",
       "      <td>36.435763</td>\n",
       "      <td>14.496814</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>149.707718</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410108</td>\n",
       "      <td>0.405114</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>1</td>\n",
       "      <td>2.058941</td>\n",
       "      <td>1</td>\n",
       "      <td>81.099707</td>\n",
       "      <td>0</td>\n",
       "      <td>4.444450</td>\n",
       "      <td>0</td>\n",
       "      <td>69.920669</td>\n",
       "      <td>0</td>\n",
       "      <td>60.808637</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.666661</td>\n",
       "      <td>0</td>\n",
       "      <td>828264.024156</td>\n",
       "      <td>1</td>\n",
       "      <td>63.063063</td>\n",
       "      <td>50.743160</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>60.446223</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170615</td>\n",
       "      <td>1</td>\n",
       "      <td>20.633333</td>\n",
       "      <td>23.106666</td>\n",
       "      <td>20.506666</td>\n",
       "      <td>23.233333</td>\n",
       "      <td>18.033333</td>\n",
       "      <td>25.706666</td>\n",
       "      <td>17.906666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>20.719999</td>\n",
       "      <td>20.730000</td>\n",
       "      <td>20.270000</td>\n",
       "      <td>20.340000</td>\n",
       "      <td>804600</td>\n",
       "      <td>20.260000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.085546</td>\n",
       "      <td>0</td>\n",
       "      <td>20.667400</td>\n",
       "      <td>20.0395</td>\n",
       "      <td>19.411600</td>\n",
       "      <td>0.739290</td>\n",
       "      <td>6.266626</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>19.342568</td>\n",
       "      <td>18.525142</td>\n",
       "      <td>18.481989</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.183869</td>\n",
       "      <td>18.435459</td>\n",
       "      <td>18.545405</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.969009</td>\n",
       "      <td>1</td>\n",
       "      <td>34.451424</td>\n",
       "      <td>32.362519</td>\n",
       "      <td>17.493670</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>85.708389</td>\n",
       "      <td>1</td>\n",
       "      <td>0.391124</td>\n",
       "      <td>0.402316</td>\n",
       "      <td>-0.011192</td>\n",
       "      <td>0</td>\n",
       "      <td>2.041574</td>\n",
       "      <td>1</td>\n",
       "      <td>76.927311</td>\n",
       "      <td>0</td>\n",
       "      <td>1.345291</td>\n",
       "      <td>0</td>\n",
       "      <td>60.938720</td>\n",
       "      <td>0</td>\n",
       "      <td>54.881580</td>\n",
       "      <td>0</td>\n",
       "      <td>-34.999971</td>\n",
       "      <td>0</td>\n",
       "      <td>629694.026402</td>\n",
       "      <td>1</td>\n",
       "      <td>77.477477</td>\n",
       "      <td>63.235708</td>\n",
       "      <td>0</td>\n",
       "      <td>10.627286</td>\n",
       "      <td>63.988652</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175967</td>\n",
       "      <td>1</td>\n",
       "      <td>20.446667</td>\n",
       "      <td>22.583334</td>\n",
       "      <td>20.133333</td>\n",
       "      <td>22.896668</td>\n",
       "      <td>17.996666</td>\n",
       "      <td>25.033335</td>\n",
       "      <td>17.683332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>20.340000</td>\n",
       "      <td>20.379999</td>\n",
       "      <td>20.160000</td>\n",
       "      <td>20.260000</td>\n",
       "      <td>471900</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.007404</td>\n",
       "      <td>18.570000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.083416</td>\n",
       "      <td>0</td>\n",
       "      <td>20.677310</td>\n",
       "      <td>20.0705</td>\n",
       "      <td>19.463690</td>\n",
       "      <td>0.656144</td>\n",
       "      <td>6.046787</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>19.378545</td>\n",
       "      <td>18.548120</td>\n",
       "      <td>18.499681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.249017</td>\n",
       "      <td>18.462442</td>\n",
       "      <td>18.574249</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.048108</td>\n",
       "      <td>1</td>\n",
       "      <td>33.614615</td>\n",
       "      <td>30.600584</td>\n",
       "      <td>19.263447</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>24.268894</td>\n",
       "      <td>1</td>\n",
       "      <td>0.365411</td>\n",
       "      <td>0.394935</td>\n",
       "      <td>-0.029524</td>\n",
       "      <td>0</td>\n",
       "      <td>1.902912</td>\n",
       "      <td>1</td>\n",
       "      <td>72.638398</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.246179</td>\n",
       "      <td>0</td>\n",
       "      <td>59.017652</td>\n",
       "      <td>0</td>\n",
       "      <td>53.230087</td>\n",
       "      <td>0</td>\n",
       "      <td>-41.666632</td>\n",
       "      <td>0</td>\n",
       "      <td>477566.554469</td>\n",
       "      <td>1</td>\n",
       "      <td>54.954955</td>\n",
       "      <td>65.165165</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.875762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.180626</td>\n",
       "      <td>1</td>\n",
       "      <td>20.266666</td>\n",
       "      <td>22.223334</td>\n",
       "      <td>19.773333</td>\n",
       "      <td>22.716667</td>\n",
       "      <td>17.816665</td>\n",
       "      <td>24.673335</td>\n",
       "      <td>17.323332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>20.020000</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>549100</td>\n",
       "      <td>20.150000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.085032</td>\n",
       "      <td>0</td>\n",
       "      <td>20.670872</td>\n",
       "      <td>20.0925</td>\n",
       "      <td>19.514128</td>\n",
       "      <td>0.515130</td>\n",
       "      <td>5.757097</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>19.407230</td>\n",
       "      <td>18.568807</td>\n",
       "      <td>18.515704</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.296817</td>\n",
       "      <td>18.485820</td>\n",
       "      <td>18.599729</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.760000</td>\n",
       "      <td>0</td>\n",
       "      <td>32.227258</td>\n",
       "      <td>28.760971</td>\n",
       "      <td>21.612208</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>-45.787970</td>\n",
       "      <td>1</td>\n",
       "      <td>0.329136</td>\n",
       "      <td>0.381775</td>\n",
       "      <td>-0.052639</td>\n",
       "      <td>0</td>\n",
       "      <td>1.685848</td>\n",
       "      <td>1</td>\n",
       "      <td>70.597898</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.049702</td>\n",
       "      <td>0</td>\n",
       "      <td>55.485710</td>\n",
       "      <td>0</td>\n",
       "      <td>53.570511</td>\n",
       "      <td>0</td>\n",
       "      <td>-54.621765</td>\n",
       "      <td>0</td>\n",
       "      <td>333937.741315</td>\n",
       "      <td>1</td>\n",
       "      <td>29.279324</td>\n",
       "      <td>53.903919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.542429</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184391</td>\n",
       "      <td>1</td>\n",
       "      <td>20.126667</td>\n",
       "      <td>21.943335</td>\n",
       "      <td>19.493334</td>\n",
       "      <td>22.576668</td>\n",
       "      <td>17.676666</td>\n",
       "      <td>24.393336</td>\n",
       "      <td>17.043333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       open       high        low      close   volume    close_1 result_1    perf_1   close_14 result_14   perf_14  results   bb_upper  bb_middle   bb_lower    bb_pct  bb_bandwidth bb_squeeze bb_signalup bb_signaldn  bb_signal      ema50     ema150     ema200  ema_signal1  ema_signal2     kama50    kama150    kama200  kama_signal1  kama_signal2        sar  sar_signal        adx    plus_di   minus_di adx_trend adx_direction  adx_signal  aroon_osc  aroon_signal         cci  \\\n",
       "0  2010-01-04  20.370001  20.549999  20.270000  20.549999   664200  20.680000     True  0.006326  18.719999     False -0.089051        0  20.545079    19.9480  19.350920  1.004120      5.986360      False        True       False          0  19.245605  18.471530  18.441038            1            1  19.030765  18.366468  18.470379             1             1  19.822958           1  34.171492  33.666344  15.692957      True          True           1         96             1  122.917840   \n",
       "1  2010-01-05  20.610001  20.760000  20.459999  20.680000  1214800  20.340000    False -0.016441  18.900000     False -0.086074        0  20.632786    20.0125  19.392214  1.038058      6.198986      False        True       False          0  19.301856  18.500781  18.463316            1            1  19.120883  18.404560  18.516717             1             1  19.881121           1  34.807420  36.435763  14.496814      True          True           1        100             1  149.707718   \n",
       "2  2010-01-06  20.719999  20.730000  20.270000  20.340000   804600  20.260000    False -0.003933  18.600000     False -0.085546        0  20.667400    20.0395  19.411600  0.739290      6.266626      False       False       False          0  19.342568  18.525142  18.481989            1            1  19.183869  18.435459  18.545405             1             1  19.969009           1  34.451424  32.362519  17.493670      True          True           1         96             1   85.708389   \n",
       "3  2010-01-07  20.340000  20.379999  20.160000  20.260000   471900  20.110001    False -0.007404  18.570000     False -0.083416        0  20.677310    20.0705  19.463690  0.656144      6.046787      False       False       False          0  19.378545  18.548120  18.499681            1            1  19.249017  18.462442  18.574249             1             1  20.048108           1  33.614615  30.600584  19.263447      True          True           1         92             1   24.268894   \n",
       "4  2010-01-08  20.250000  20.250000  20.020000  20.110001   549100  20.150000     True  0.001989  18.400000     False -0.085032        0  20.670872    20.0925  19.514128  0.515130      5.757097      False       False       False          0  19.407230  18.568807  18.515704            1            1  19.296817  18.485820  18.599729             1             1  20.760000           0  32.227258  28.760971  21.612208      True          True           1         88             1  -45.787970   \n",
       "\n",
       "   cci_signal      macd  macd_sigline  macd_hist  macd_signal       ppo  ppo_signal        mfi  mfi_signal       roc  roc_signal        rsi  rsi_signal    ult_osc  ult_signal      willr  wr_signal         ad_osc  ad_signal  stoch_slowk  stoch_slowd  sslow_signal  stoch_fastk  stoch_fastd  srsi_signal      trix  trix_signal  sr_pivotpts    sr_res1    sr_sup1    sr_res2    sr_sup2    sr_res3    sr_sup3  cv_signal  \n",
       "0           1  0.393756      0.403865  -0.010110            0  2.048084           1  79.657038           0  4.900454           0  68.259660           0  56.239960           0  -0.000000          0  694259.549529          1    49.166583    52.670801             0    81.338670    27.112890            0  0.164700            1    20.456666  22.773333  20.363333  22.866666  18.046666  25.183333  17.953333          0  \n",
       "1           1  0.410108      0.405114   0.004994            1  2.058941           1  81.099707           0  4.444450           0  69.920669           0  60.808637           0  -6.666661          0  828264.024156          1    63.063063    50.743160             0   100.000000    60.446223            0  0.170615            1    20.633333  23.106666  20.506666  23.233333  18.033333  25.706666  17.906666          0  \n",
       "2           1  0.391124      0.402316  -0.011192            0  2.041574           1  76.927311           0  1.345291           0  60.938720           0  54.881580           0 -34.999971          0  629694.026402          1    77.477477    63.235708             0    10.627286    63.988652            1  0.175967            1    20.446667  22.583334  20.133333  22.896668  17.996666  25.033335  17.683332          0  \n",
       "3           1  0.365411      0.394935  -0.029524            0  1.902912           1  72.638398           0 -0.246179           0  59.017652           0  53.230087           0 -41.666632          0  477566.554469          1    54.954955    65.165165             0     0.000000    36.875762            1  0.180626            1    20.266666  22.223334  19.773333  22.716667  17.816665  24.673335  17.323332          1  \n",
       "4           1  0.329136      0.381775  -0.052639            0  1.685848           1  70.597898           0 -0.049702           0  55.485710           0  53.570511           0 -54.621765          0  333937.741315          1    29.279324    53.903919             0     0.000000     3.542429            1  0.184391            1    20.126667  21.943335  19.493334  22.576668  17.676666  24.393336  17.043333          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=pd.read_csv(\"data/IYZ.csv\")\n",
    "ticker = 'IYZ'\n",
    "startdate=datetime.date(2010, 1, 1)\n",
    "enddate=datetime.date.today()\n",
    "df = generate_ticker_data(ticker, startdate, enddate)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_1</th>\n",
       "      <th>result_1</th>\n",
       "      <th>perf_1</th>\n",
       "      <th>close_14</th>\n",
       "      <th>result_14</th>\n",
       "      <th>perf_14</th>\n",
       "      <th>results</th>\n",
       "      <th>bb_upper</th>\n",
       "      <th>bb_middle</th>\n",
       "      <th>bb_lower</th>\n",
       "      <th>bb_pct</th>\n",
       "      <th>bb_bandwidth</th>\n",
       "      <th>bb_squeeze</th>\n",
       "      <th>bb_signalup</th>\n",
       "      <th>bb_signaldn</th>\n",
       "      <th>bb_signal</th>\n",
       "      <th>ema50</th>\n",
       "      <th>ema150</th>\n",
       "      <th>ema200</th>\n",
       "      <th>ema_signal1</th>\n",
       "      <th>ema_signal2</th>\n",
       "      <th>kama50</th>\n",
       "      <th>kama150</th>\n",
       "      <th>kama200</th>\n",
       "      <th>kama_signal1</th>\n",
       "      <th>kama_signal2</th>\n",
       "      <th>sar</th>\n",
       "      <th>sar_signal</th>\n",
       "      <th>adx</th>\n",
       "      <th>plus_di</th>\n",
       "      <th>minus_di</th>\n",
       "      <th>adx_trend</th>\n",
       "      <th>adx_direction</th>\n",
       "      <th>adx_signal</th>\n",
       "      <th>aroon_osc</th>\n",
       "      <th>aroon_signal</th>\n",
       "      <th>cci</th>\n",
       "      <th>cci_signal</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_sigline</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>ppo</th>\n",
       "      <th>ppo_signal</th>\n",
       "      <th>mfi</th>\n",
       "      <th>mfi_signal</th>\n",
       "      <th>roc</th>\n",
       "      <th>roc_signal</th>\n",
       "      <th>rsi</th>\n",
       "      <th>rsi_signal</th>\n",
       "      <th>ult_osc</th>\n",
       "      <th>ult_signal</th>\n",
       "      <th>willr</th>\n",
       "      <th>wr_signal</th>\n",
       "      <th>ad_osc</th>\n",
       "      <th>ad_signal</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>sslow_signal</th>\n",
       "      <th>stoch_fastk</th>\n",
       "      <th>stoch_fastd</th>\n",
       "      <th>srsi_signal</th>\n",
       "      <th>trix</th>\n",
       "      <th>trix_signal</th>\n",
       "      <th>sr_pivotpts</th>\n",
       "      <th>sr_res1</th>\n",
       "      <th>sr_sup1</th>\n",
       "      <th>sr_res2</th>\n",
       "      <th>sr_sup2</th>\n",
       "      <th>sr_res3</th>\n",
       "      <th>sr_sup3</th>\n",
       "      <th>cv_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>2015-11-20</td>\n",
       "      <td>29.860001</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>29.639999</td>\n",
       "      <td>29.680000</td>\n",
       "      <td>153400</td>\n",
       "      <td>29.629999</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.001685</td>\n",
       "      <td>62.43750</td>\n",
       "      <td>True</td>\n",
       "      <td>1.103689</td>\n",
       "      <td>0</td>\n",
       "      <td>30.489814</td>\n",
       "      <td>29.7270</td>\n",
       "      <td>28.964186</td>\n",
       "      <td>0.469193</td>\n",
       "      <td>5.132129</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>29.162897</td>\n",
       "      <td>29.227979</td>\n",
       "      <td>29.312981</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.013116</td>\n",
       "      <td>29.253619</td>\n",
       "      <td>29.432630</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.890592</td>\n",
       "      <td>1</td>\n",
       "      <td>16.913323</td>\n",
       "      <td>25.572952</td>\n",
       "      <td>20.355045</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>17.310571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163693</td>\n",
       "      <td>0.224167</td>\n",
       "      <td>-0.060474</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.167344</td>\n",
       "      <td>0</td>\n",
       "      <td>58.479730</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>0</td>\n",
       "      <td>54.495110</td>\n",
       "      <td>1</td>\n",
       "      <td>54.499672</td>\n",
       "      <td>0</td>\n",
       "      <td>-51.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>385091.955069</td>\n",
       "      <td>1</td>\n",
       "      <td>88.577258</td>\n",
       "      <td>80.148257</td>\n",
       "      <td>0</td>\n",
       "      <td>72.755132</td>\n",
       "      <td>90.918377</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062966</td>\n",
       "      <td>1</td>\n",
       "      <td>29.740000</td>\n",
       "      <td>31.510000</td>\n",
       "      <td>28.899999</td>\n",
       "      <td>32.350001</td>\n",
       "      <td>27.129999</td>\n",
       "      <td>34.120001</td>\n",
       "      <td>26.289998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>2015-11-23</td>\n",
       "      <td>29.690001</td>\n",
       "      <td>29.840000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>29.629999</td>\n",
       "      <td>288200</td>\n",
       "      <td>29.760000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>62.46875</td>\n",
       "      <td>True</td>\n",
       "      <td>1.108294</td>\n",
       "      <td>0</td>\n",
       "      <td>30.481659</td>\n",
       "      <td>29.7185</td>\n",
       "      <td>28.955341</td>\n",
       "      <td>0.442017</td>\n",
       "      <td>5.135921</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>29.181215</td>\n",
       "      <td>29.233304</td>\n",
       "      <td>29.316135</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.022509</td>\n",
       "      <td>29.256816</td>\n",
       "      <td>29.433652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.951157</td>\n",
       "      <td>1</td>\n",
       "      <td>16.326191</td>\n",
       "      <td>24.382011</td>\n",
       "      <td>20.481786</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>17.048184</td>\n",
       "      <td>1</td>\n",
       "      <td>0.155769</td>\n",
       "      <td>0.210487</td>\n",
       "      <td>-0.054719</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385339</td>\n",
       "      <td>0</td>\n",
       "      <td>43.285668</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.470275</td>\n",
       "      <td>0</td>\n",
       "      <td>53.636401</td>\n",
       "      <td>1</td>\n",
       "      <td>53.670164</td>\n",
       "      <td>0</td>\n",
       "      <td>-54.285771</td>\n",
       "      <td>1</td>\n",
       "      <td>284944.334474</td>\n",
       "      <td>1</td>\n",
       "      <td>77.700214</td>\n",
       "      <td>82.962588</td>\n",
       "      <td>0</td>\n",
       "      <td>49.788373</td>\n",
       "      <td>74.181168</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064829</td>\n",
       "      <td>1</td>\n",
       "      <td>29.683333</td>\n",
       "      <td>31.396667</td>\n",
       "      <td>28.786666</td>\n",
       "      <td>32.293334</td>\n",
       "      <td>27.073332</td>\n",
       "      <td>34.006668</td>\n",
       "      <td>26.176665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>2015-11-24</td>\n",
       "      <td>29.549999</td>\n",
       "      <td>29.809999</td>\n",
       "      <td>29.420000</td>\n",
       "      <td>29.760000</td>\n",
       "      <td>558700</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.003024</td>\n",
       "      <td>62.00000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>30.485201</td>\n",
       "      <td>29.7370</td>\n",
       "      <td>28.988799</td>\n",
       "      <td>0.515370</td>\n",
       "      <td>5.032123</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>29.203912</td>\n",
       "      <td>29.240280</td>\n",
       "      <td>29.320552</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.032014</td>\n",
       "      <td>29.261284</td>\n",
       "      <td>29.435428</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.008087</td>\n",
       "      <td>1</td>\n",
       "      <td>15.281278</td>\n",
       "      <td>22.676114</td>\n",
       "      <td>21.919152</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>26.554818</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158155</td>\n",
       "      <td>0.200021</td>\n",
       "      <td>-0.041866</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.575353</td>\n",
       "      <td>0</td>\n",
       "      <td>41.692550</td>\n",
       "      <td>0</td>\n",
       "      <td>0.642543</td>\n",
       "      <td>0</td>\n",
       "      <td>55.595551</td>\n",
       "      <td>1</td>\n",
       "      <td>62.117478</td>\n",
       "      <td>0</td>\n",
       "      <td>-32.116838</td>\n",
       "      <td>1</td>\n",
       "      <td>350258.214409</td>\n",
       "      <td>1</td>\n",
       "      <td>73.218502</td>\n",
       "      <td>79.831991</td>\n",
       "      <td>0</td>\n",
       "      <td>77.479020</td>\n",
       "      <td>66.674175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066542</td>\n",
       "      <td>1</td>\n",
       "      <td>29.663333</td>\n",
       "      <td>31.356667</td>\n",
       "      <td>28.746666</td>\n",
       "      <td>32.273334</td>\n",
       "      <td>27.053332</td>\n",
       "      <td>33.966668</td>\n",
       "      <td>26.136665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>2015-11-25</td>\n",
       "      <td>29.770000</td>\n",
       "      <td>29.770000</td>\n",
       "      <td>29.590000</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>351900</td>\n",
       "      <td>29.780001</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>63.06250</td>\n",
       "      <td>True</td>\n",
       "      <td>1.125463</td>\n",
       "      <td>0</td>\n",
       "      <td>30.449752</td>\n",
       "      <td>29.7170</td>\n",
       "      <td>28.984248</td>\n",
       "      <td>0.467929</td>\n",
       "      <td>4.931532</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>29.222190</td>\n",
       "      <td>29.245972</td>\n",
       "      <td>29.324029</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.038808</td>\n",
       "      <td>29.264858</td>\n",
       "      <td>29.436786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.061602</td>\n",
       "      <td>1</td>\n",
       "      <td>14.311001</td>\n",
       "      <td>21.914036</td>\n",
       "      <td>21.182514</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>-32</td>\n",
       "      <td>0</td>\n",
       "      <td>39.583001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151043</td>\n",
       "      <td>0.190225</td>\n",
       "      <td>-0.039182</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.652351</td>\n",
       "      <td>0</td>\n",
       "      <td>48.895610</td>\n",
       "      <td>0</td>\n",
       "      <td>0.884053</td>\n",
       "      <td>0</td>\n",
       "      <td>53.897542</td>\n",
       "      <td>1</td>\n",
       "      <td>56.955565</td>\n",
       "      <td>0</td>\n",
       "      <td>-28.813559</td>\n",
       "      <td>1</td>\n",
       "      <td>332694.963982</td>\n",
       "      <td>1</td>\n",
       "      <td>64.099862</td>\n",
       "      <td>71.672859</td>\n",
       "      <td>0</td>\n",
       "      <td>10.327409</td>\n",
       "      <td>45.864934</td>\n",
       "      <td>1</td>\n",
       "      <td>0.068008</td>\n",
       "      <td>1</td>\n",
       "      <td>29.676667</td>\n",
       "      <td>31.263333</td>\n",
       "      <td>28.773333</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>27.186667</td>\n",
       "      <td>33.753333</td>\n",
       "      <td>26.283333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>2015-11-27</td>\n",
       "      <td>29.650000</td>\n",
       "      <td>29.879999</td>\n",
       "      <td>29.650000</td>\n",
       "      <td>29.780001</td>\n",
       "      <td>99100</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.880457</td>\n",
       "      <td>63.18750</td>\n",
       "      <td>True</td>\n",
       "      <td>1.121810</td>\n",
       "      <td>0</td>\n",
       "      <td>30.451038</td>\n",
       "      <td>29.7180</td>\n",
       "      <td>28.984962</td>\n",
       "      <td>0.542290</td>\n",
       "      <td>4.933295</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>29.244065</td>\n",
       "      <td>29.253045</td>\n",
       "      <td>29.328566</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.046764</td>\n",
       "      <td>29.268790</td>\n",
       "      <td>29.438804</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.111906</td>\n",
       "      <td>1</td>\n",
       "      <td>13.752790</td>\n",
       "      <td>23.059373</td>\n",
       "      <td>20.246219</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>-32</td>\n",
       "      <td>0</td>\n",
       "      <td>72.673244</td>\n",
       "      <td>1</td>\n",
       "      <td>0.152525</td>\n",
       "      <td>0.182685</td>\n",
       "      <td>-0.030160</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.698883</td>\n",
       "      <td>0</td>\n",
       "      <td>51.173862</td>\n",
       "      <td>0</td>\n",
       "      <td>2.126204</td>\n",
       "      <td>0</td>\n",
       "      <td>55.679286</td>\n",
       "      <td>1</td>\n",
       "      <td>56.826208</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.803333</td>\n",
       "      <td>0</td>\n",
       "      <td>299378.276190</td>\n",
       "      <td>1</td>\n",
       "      <td>68.623807</td>\n",
       "      <td>68.647390</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>62.602143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069339</td>\n",
       "      <td>1</td>\n",
       "      <td>29.770000</td>\n",
       "      <td>30.980001</td>\n",
       "      <td>28.960000</td>\n",
       "      <td>31.790001</td>\n",
       "      <td>27.749999</td>\n",
       "      <td>33.000002</td>\n",
       "      <td>26.939999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date       open       high        low      close  volume    close_1 result_1    perf_1  close_14 result_14   perf_14  results   bb_upper  bb_middle   bb_lower    bb_pct  bb_bandwidth bb_squeeze bb_signalup bb_signaldn  bb_signal      ema50     ema150     ema200  ema_signal1  ema_signal2     kama50    kama150    kama200  kama_signal1  kama_signal2        sar  sar_signal        adx    plus_di   minus_di adx_trend adx_direction  adx_signal  aroon_osc  aroon_signal        cci  \\\n",
       "1482  2015-11-20  29.860001  29.900000  29.639999  29.680000  153400  29.629999    False -0.001685  62.43750      True  1.103689        0  30.489814    29.7270  28.964186  0.469193      5.132129      False       False       False          0  29.162897  29.227979  29.312981            1            0  29.013116  29.253619  29.432630             1             0  28.890592           1  16.913323  25.572952  20.355045     False          True           1         52             1  17.310571   \n",
       "1483  2015-11-23  29.690001  29.840000  29.580000  29.629999  288200  29.760000     True  0.004387  62.46875      True  1.108294        0  30.481659    29.7185  28.955341  0.442017      5.135921      False       False       False          0  29.181215  29.233304  29.316135            1            0  29.022509  29.256816  29.433652             1             0  28.951157           1  16.326191  24.382011  20.481786     False          True           1         48             1  17.048184   \n",
       "1484  2015-11-24  29.549999  29.809999  29.420000  29.760000  558700  29.670000    False -0.003024  62.00000      True  1.083333        0  30.485201    29.7370  28.988799  0.515370      5.032123      False       False       False          0  29.203912  29.240280  29.320552            1            0  29.032014  29.261284  29.435428             1             0  29.008087           1  15.281278  22.676114  21.919152     False          True           1         44             1  26.554818   \n",
       "1485  2015-11-25  29.770000  29.770000  29.590000  29.670000  351900  29.780001     True  0.003707  63.06250      True  1.125463        0  30.449752    29.7170  28.984248  0.467929      4.931532      False       False       False          0  29.222190  29.245972  29.324029            1            0  29.038808  29.264858  29.436786             1             0  29.061602           1  14.311001  21.914036  21.182514     False          True           1        -32             0  39.583001   \n",
       "1486  2015-11-27  29.650000  29.879999  29.650000  29.780001   99100  56.000000     True  0.880457  63.18750      True  1.121810        0  30.451038    29.7180  28.984962  0.542290      4.933295      False       False       False          0  29.244065  29.253045  29.328566            1            0  29.046764  29.268790  29.438804             1             0  29.111906           1  13.752790  23.059373  20.246219     False          True           1        -32             0  72.673244   \n",
       "\n",
       "      cci_signal      macd  macd_sigline  macd_hist  macd_signal       ppo  ppo_signal        mfi  mfi_signal       roc  roc_signal        rsi  rsi_signal    ult_osc  ult_signal      willr  wr_signal         ad_osc  ad_signal  stoch_slowk  stoch_slowd  sslow_signal  stoch_fastk  stoch_fastd  srsi_signal      trix  trix_signal  sr_pivotpts    sr_res1    sr_sup1    sr_res2    sr_sup2    sr_res3    sr_sup3  cv_signal  \n",
       "1482           1  0.163693      0.224167  -0.060474            0 -0.167344           0  58.479730           0 -1.066667           0  54.495110           1  54.499672           0 -51.428571          1  385091.955069          1    88.577258    80.148257             0    72.755132    90.918377            0  0.062966            1    29.740000  31.510000  28.899999  32.350001  27.129999  34.120001  26.289998          0  \n",
       "1483           1  0.155769      0.210487  -0.054719            0 -0.385339           0  43.285668           0 -0.470275           0  53.636401           1  53.670164           0 -54.285771          1  284944.334474          1    77.700214    82.962588             0    49.788373    74.181168            0  0.064829            1    29.683333  31.396667  28.786666  32.293334  27.073332  34.006668  26.176665          0  \n",
       "1484           1  0.158155      0.200021  -0.041866            0 -0.575353           0  41.692550           0  0.642543           0  55.595551           1  62.117478           0 -32.116838          1  350258.214409          1    73.218502    79.831991             0    77.479020    66.674175            0  0.066542            1    29.663333  31.356667  28.746666  32.273334  27.053332  33.966668  26.136665          0  \n",
       "1485           1  0.151043      0.190225  -0.039182            0 -0.652351           0  48.895610           0  0.884053           0  53.897542           1  56.955565           0 -28.813559          1  332694.963982          1    64.099862    71.672859             0    10.327409    45.864934            1  0.068008            1    29.676667  31.263333  28.773333  32.166667  27.186667  33.753333  26.283333          0  \n",
       "1486           1  0.152525      0.182685  -0.030160            0 -0.698883           0  51.173862           0  2.126204           0  55.679286           1  56.826208           0 -18.803333          0  299378.276190          1    68.623807    68.647390             0   100.000000    62.602143            0  0.069339            1    29.770000  30.980001  28.960000  31.790001  27.749999  33.000002  26.939999          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftouse=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IGNORE = ['date', 'result_1','close_1','perf_1','result_14','close_14','perf_14','results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb_squeeze [False True]\n",
      "bb_signalup [True False]\n",
      "bb_signaldn [False True]\n",
      "bb_signal [ 0.  1.]\n",
      "ema_signal1 [1 0]\n",
      "ema_signal2 [1 0]\n",
      "kama_signal1 [1 0]\n",
      "kama_signal2 [1 0]\n",
      "sar_signal [1 0]\n",
      "adx_trend [True False]\n",
      "adx_direction [True False]\n",
      "adx_signal [ 1.  0.]\n",
      "aroon_signal [1 0]\n",
      "cci_signal [ 1.  0.]\n",
      "macd_signal [0 1]\n",
      "ppo_signal [1 0]\n",
      "mfi_signal [ 0.  1.]\n",
      "roc_signal [ 0.  1.]\n",
      "rsi_signal [ 0.  1.]\n",
      "ult_signal [ 0.  1.]\n",
      "wr_signal [ 0.  1.]\n",
      "ad_signal [1 0]\n",
      "sslow_signal [ 0.  1.]\n",
      "srsi_signal [ 0.  1.]\n",
      "trix_signal [1 0]\n",
      "cv_signal [0 1]\n"
     ]
    }
   ],
   "source": [
    "INDICATORS=[]\n",
    "for v in df.columns:\n",
    "    l=df[v].unique()\n",
    "    if len(l) <= 10 and v not in IGNORE:\n",
    "        print v, l\n",
    "        INDICATORS.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open\n",
      "high\n",
      "low\n",
      "close\n",
      "volume\n",
      "bb_upper\n",
      "bb_middle\n",
      "bb_lower\n",
      "bb_pct\n",
      "bb_bandwidth\n",
      "ema50\n",
      "ema150\n",
      "ema200\n",
      "kama50\n",
      "kama150\n",
      "kama200\n",
      "sar\n",
      "adx\n",
      "plus_di\n",
      "minus_di\n",
      "aroon_osc\n",
      "cci\n",
      "macd\n",
      "macd_sigline\n",
      "macd_hist\n",
      "ppo\n",
      "mfi\n",
      "roc\n",
      "rsi\n",
      "ult_osc\n",
      "willr\n",
      "ad_osc\n",
      "stoch_slowk\n",
      "stoch_slowd\n",
      "stoch_fastk\n",
      "stoch_fastd\n",
      "trix\n",
      "sr_pivotpts\n",
      "sr_res1\n",
      "sr_sup1\n",
      "sr_res2\n",
      "sr_sup2\n",
      "sr_res3\n",
      "sr_sup3\n"
     ]
    }
   ],
   "source": [
    "STANDARDIZABLE = []\n",
    "for v in df.columns:\n",
    "    if v not in INDICATORS and v not in IGNORE:\n",
    "        print v\n",
    "        STANDARDIZABLE.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import train_test_split\n",
    "# itrain, itest = train_test_split(xrange(dftouse.shape[0]), train_size=0.7)\n",
    "# mask=np.ones(dftouse.shape[0], dtype='int')\n",
    "# mask[itrain]=1\n",
    "# mask[itest]=0\n",
    "# mask = (mask==1)\n",
    "# mask.shape, mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1487,), 1258)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftouse['date'] = pd.to_datetime(dftouse['date'])\n",
    "mask = (dftouse.date < '2015-01-01').values\n",
    "mask.shape, mask.sum()\n",
    "\n",
    "#mask = dftouse['date'] < datetime.date(2014, 1, 1)\n",
    "#maskv = (dftouse['date'] > datetime.date(2013, 12, 31)) & (dftouse['date'] < datetime.date(2015, 1, 1))\n",
    "#maskt = dftouse['date'] > datetime.date(2014, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check ROI of signals, alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print \"ROI baseline: 1.12%\"\n",
    "#print 'ROI \"result\" buy-only: 67.45%'\n",
    "#print 'ROI \"result\" buy-sell: 172.49%'\n",
    "    \n",
    "def signalperf(signal):\n",
    "    ypred = df[signal]\n",
    "    df_pred = df[~mask].reset_index(drop=True)\n",
    "    df_pred['pred_result'] = ypred\n",
    "    df_pred['result_baseline'] = np.ones(df_pred.shape[0])\n",
    "\n",
    "    balance, profit, ROI2, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'pred_result', 'close', False, [1])\n",
    "    #print 'ROI \"pred\" buy-only: {0:.2f}%'.format(ROI*100)\n",
    "\n",
    "    balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'pred_result', 'close', False, [0,1])\n",
    "    print signal + ': ROI \"pred\" buy-only: {0:.2f}%'.format(ROI2*100), 'buy-sell: {0:.2f}%'.format(ROI*100)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb_squeeze: ROI \"pred\" buy-only: 0.83% buy-sell: -0.80%\n",
      "bb_signalup: ROI \"pred\" buy-only: -9.98% buy-sell: -21.18%\n",
      "bb_signaldn: ROI \"pred\" buy-only: -1.77% buy-sell: -5.17%\n",
      "bb_signal: ROI \"pred\" buy-only: -2.03% buy-sell: -5.54%\n",
      "ema_signal1: ROI \"pred\" buy-only: 0.46% buy-sell: -0.92%\n",
      "ema_signal2: ROI \"pred\" buy-only: 1.46% buy-sell: 1.46%\n",
      "kama_signal1: ROI \"pred\" buy-only: 2.05% buy-sell: 2.28%\n",
      "kama_signal2: ROI \"pred\" buy-only: 1.46% buy-sell: 1.46%\n",
      "sar_signal: ROI \"pred\" buy-only: -9.83% buy-sell: -20.55%\n",
      "adx_trend: ROI \"pred\" buy-only: 12.65% buy-sell: 24.43%\n",
      "adx_direction: ROI \"pred\" buy-only: 1.26% buy-sell: -0.14%\n",
      "adx_signal: ROI \"pred\" buy-only: -0.99% buy-sell: -4.25%\n",
      "aroon_signal: ROI \"pred\" buy-only: -2.21% buy-sell: -6.47%\n",
      "cci_signal: ROI \"pred\" buy-only: -4.73% buy-sell: -10.91%\n",
      "macd_signal: ROI \"pred\" buy-only: -3.20% buy-sell: -9.60%\n",
      "ppo_signal: ROI \"pred\" buy-only: -5.03% buy-sell: -11.49%\n",
      "mfi_signal: ROI \"pred\" buy-only: -3.06% buy-sell: -7.56%\n",
      "roc_signal: ROI \"pred\" buy-only: 0.00% buy-sell: -1.46%\n",
      "rsi_signal: ROI \"pred\" buy-only: 0.42% buy-sell: -0.91%\n",
      "ult_signal: ROI \"pred\" buy-only: 0.00% buy-sell: -1.46%\n",
      "wr_signal: ROI \"pred\" buy-only: 10.72% buy-sell: 20.14%\n",
      "ad_signal: ROI \"pred\" buy-only: -1.44% buy-sell: -5.35%\n",
      "sslow_signal: ROI \"pred\" buy-only: 3.51% buy-sell: 5.16%\n",
      "srsi_signal: ROI \"pred\" buy-only: 5.22% buy-sell: 8.36%\n",
      "trix_signal: ROI \"pred\" buy-only: 4.81% buy-sell: 8.17%\n",
      "cv_signal: ROI \"pred\" buy-only: 0.18% buy-sell: -2.72%\n"
     ]
    }
   ],
   "source": [
    "for v in INDICATORS:\n",
    "    signalperf(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the mask to compute the training and test parts of the dataframe. Use `StandardScaler` from `sklearn.preprocessing` to \"fit\" the columns in `STANDARDIZABLE` on the training set. Then use the resultant estimator to transform both the training and the test parts of each of the columns in the dataframe, replacing the old unstandardized values in the `STANDARDIZABLE` columns of `dftouse` by the new standardized ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_1</th>\n",
       "      <th>result_1</th>\n",
       "      <th>perf_1</th>\n",
       "      <th>close_14</th>\n",
       "      <th>result_14</th>\n",
       "      <th>perf_14</th>\n",
       "      <th>results</th>\n",
       "      <th>bb_upper</th>\n",
       "      <th>bb_middle</th>\n",
       "      <th>bb_lower</th>\n",
       "      <th>bb_pct</th>\n",
       "      <th>bb_bandwidth</th>\n",
       "      <th>bb_squeeze</th>\n",
       "      <th>bb_signalup</th>\n",
       "      <th>bb_signaldn</th>\n",
       "      <th>bb_signal</th>\n",
       "      <th>ema50</th>\n",
       "      <th>ema150</th>\n",
       "      <th>ema200</th>\n",
       "      <th>ema_signal1</th>\n",
       "      <th>ema_signal2</th>\n",
       "      <th>kama50</th>\n",
       "      <th>kama150</th>\n",
       "      <th>kama200</th>\n",
       "      <th>kama_signal1</th>\n",
       "      <th>kama_signal2</th>\n",
       "      <th>sar</th>\n",
       "      <th>sar_signal</th>\n",
       "      <th>adx</th>\n",
       "      <th>plus_di</th>\n",
       "      <th>minus_di</th>\n",
       "      <th>adx_trend</th>\n",
       "      <th>adx_direction</th>\n",
       "      <th>adx_signal</th>\n",
       "      <th>aroon_osc</th>\n",
       "      <th>aroon_signal</th>\n",
       "      <th>cci</th>\n",
       "      <th>cci_signal</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_sigline</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>ppo</th>\n",
       "      <th>ppo_signal</th>\n",
       "      <th>mfi</th>\n",
       "      <th>mfi_signal</th>\n",
       "      <th>roc</th>\n",
       "      <th>roc_signal</th>\n",
       "      <th>rsi</th>\n",
       "      <th>rsi_signal</th>\n",
       "      <th>ult_osc</th>\n",
       "      <th>ult_signal</th>\n",
       "      <th>willr</th>\n",
       "      <th>wr_signal</th>\n",
       "      <th>ad_osc</th>\n",
       "      <th>ad_signal</th>\n",
       "      <th>stoch_slowk</th>\n",
       "      <th>stoch_slowd</th>\n",
       "      <th>sslow_signal</th>\n",
       "      <th>stoch_fastk</th>\n",
       "      <th>stoch_fastd</th>\n",
       "      <th>srsi_signal</th>\n",
       "      <th>trix</th>\n",
       "      <th>trix_signal</th>\n",
       "      <th>sr_pivotpts</th>\n",
       "      <th>sr_res1</th>\n",
       "      <th>sr_sup1</th>\n",
       "      <th>sr_res2</th>\n",
       "      <th>sr_sup2</th>\n",
       "      <th>sr_res3</th>\n",
       "      <th>sr_sup3</th>\n",
       "      <th>cv_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>-1.169181</td>\n",
       "      <td>-1.157299</td>\n",
       "      <td>-1.15216</td>\n",
       "      <td>-1.118360</td>\n",
       "      <td>0.639008</td>\n",
       "      <td>20.680000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>18.719999</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.089051</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.321332</td>\n",
       "      <td>-1.279148</td>\n",
       "      <td>-1.221656</td>\n",
       "      <td>1.341702</td>\n",
       "      <td>-0.221542</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.460919</td>\n",
       "      <td>-1.606757</td>\n",
       "      <td>-1.590867</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.591087</td>\n",
       "      <td>-1.687069</td>\n",
       "      <td>-1.676153</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.257753</td>\n",
       "      <td>1</td>\n",
       "      <td>1.219553</td>\n",
       "      <td>1.264466</td>\n",
       "      <td>-1.068669</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.313898</td>\n",
       "      <td>1</td>\n",
       "      <td>1.004782</td>\n",
       "      <td>1</td>\n",
       "      <td>1.390227</td>\n",
       "      <td>1.549988</td>\n",
       "      <td>-0.096380</td>\n",
       "      <td>0</td>\n",
       "      <td>1.164215</td>\n",
       "      <td>1</td>\n",
       "      <td>1.561731</td>\n",
       "      <td>0</td>\n",
       "      <td>1.442156</td>\n",
       "      <td>0</td>\n",
       "      <td>1.356830</td>\n",
       "      <td>0</td>\n",
       "      <td>0.191720</td>\n",
       "      <td>0</td>\n",
       "      <td>1.334246</td>\n",
       "      <td>0</td>\n",
       "      <td>1.564001</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.269844</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>0</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>-0.732738</td>\n",
       "      <td>0</td>\n",
       "      <td>1.451422</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.142985</td>\n",
       "      <td>-0.827349</td>\n",
       "      <td>-0.890331</td>\n",
       "      <td>-1.051099</td>\n",
       "      <td>-1.175919</td>\n",
       "      <td>-0.728905</td>\n",
       "      <td>-0.909411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>-1.101747</td>\n",
       "      <td>-1.098349</td>\n",
       "      <td>-1.09877</td>\n",
       "      <td>-1.081848</td>\n",
       "      <td>2.142760</td>\n",
       "      <td>20.340000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.016441</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.086074</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.296796</td>\n",
       "      <td>-1.260871</td>\n",
       "      <td>-1.209938</td>\n",
       "      <td>1.443046</td>\n",
       "      <td>-0.155418</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.444752</td>\n",
       "      <td>-1.598041</td>\n",
       "      <td>-1.584059</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.563987</td>\n",
       "      <td>-1.675790</td>\n",
       "      <td>-1.662338</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.241792</td>\n",
       "      <td>1</td>\n",
       "      <td>1.295293</td>\n",
       "      <td>1.656456</td>\n",
       "      <td>-1.216766</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.378875</td>\n",
       "      <td>1</td>\n",
       "      <td>1.253799</td>\n",
       "      <td>1</td>\n",
       "      <td>1.457677</td>\n",
       "      <td>1.555591</td>\n",
       "      <td>0.078078</td>\n",
       "      <td>1</td>\n",
       "      <td>1.171118</td>\n",
       "      <td>1</td>\n",
       "      <td>1.645223</td>\n",
       "      <td>0</td>\n",
       "      <td>1.297461</td>\n",
       "      <td>0</td>\n",
       "      <td>1.505069</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626544</td>\n",
       "      <td>0</td>\n",
       "      <td>1.120493</td>\n",
       "      <td>0</td>\n",
       "      <td>1.930265</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257237</td>\n",
       "      <td>-0.227046</td>\n",
       "      <td>0</td>\n",
       "      <td>1.139066</td>\n",
       "      <td>0.256536</td>\n",
       "      <td>0</td>\n",
       "      <td>1.519308</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.093350</td>\n",
       "      <td>-0.737175</td>\n",
       "      <td>-0.851602</td>\n",
       "      <td>-0.950513</td>\n",
       "      <td>-1.179561</td>\n",
       "      <td>-0.593715</td>\n",
       "      <td>-0.921427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>-1.070840</td>\n",
       "      <td>-1.106770</td>\n",
       "      <td>-1.15216</td>\n",
       "      <td>-1.177339</td>\n",
       "      <td>1.022456</td>\n",
       "      <td>20.260000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.085546</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.287113</td>\n",
       "      <td>-1.253220</td>\n",
       "      <td>-1.204436</td>\n",
       "      <td>0.550893</td>\n",
       "      <td>-0.134384</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.433052</td>\n",
       "      <td>-1.590782</td>\n",
       "      <td>-1.578353</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.545047</td>\n",
       "      <td>-1.666641</td>\n",
       "      <td>-1.653785</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.217674</td>\n",
       "      <td>1</td>\n",
       "      <td>1.252893</td>\n",
       "      <td>1.079920</td>\n",
       "      <td>-0.845719</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.313898</td>\n",
       "      <td>1</td>\n",
       "      <td>0.658914</td>\n",
       "      <td>1</td>\n",
       "      <td>1.379371</td>\n",
       "      <td>1.543035</td>\n",
       "      <td>-0.108883</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160075</td>\n",
       "      <td>1</td>\n",
       "      <td>1.403751</td>\n",
       "      <td>0</td>\n",
       "      <td>0.314070</td>\n",
       "      <td>0</td>\n",
       "      <td>0.703464</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062437</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212042</td>\n",
       "      <td>0</td>\n",
       "      <td>1.387529</td>\n",
       "      <td>1</td>\n",
       "      <td>0.803962</td>\n",
       "      <td>0.284918</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.973098</td>\n",
       "      <td>0.361669</td>\n",
       "      <td>1</td>\n",
       "      <td>1.580734</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.145794</td>\n",
       "      <td>-0.878748</td>\n",
       "      <td>-0.952477</td>\n",
       "      <td>-1.042869</td>\n",
       "      <td>-1.189577</td>\n",
       "      <td>-0.767654</td>\n",
       "      <td>-0.978934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>-1.177611</td>\n",
       "      <td>-1.205021</td>\n",
       "      <td>-1.18307</td>\n",
       "      <td>-1.199808</td>\n",
       "      <td>0.113814</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.007404</td>\n",
       "      <td>18.570000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.083416</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.284341</td>\n",
       "      <td>-1.244436</td>\n",
       "      <td>-1.189654</td>\n",
       "      <td>0.302612</td>\n",
       "      <td>-0.202750</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.422711</td>\n",
       "      <td>-1.583935</td>\n",
       "      <td>-1.572947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.525456</td>\n",
       "      <td>-1.658651</td>\n",
       "      <td>-1.645185</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.195968</td>\n",
       "      <td>1</td>\n",
       "      <td>1.153228</td>\n",
       "      <td>0.830531</td>\n",
       "      <td>-0.626599</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.248921</td>\n",
       "      <td>1</td>\n",
       "      <td>0.087822</td>\n",
       "      <td>1</td>\n",
       "      <td>1.273312</td>\n",
       "      <td>1.509911</td>\n",
       "      <td>-0.320626</td>\n",
       "      <td>0</td>\n",
       "      <td>1.071905</td>\n",
       "      <td>1</td>\n",
       "      <td>1.155536</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.190918</td>\n",
       "      <td>0</td>\n",
       "      <td>0.532015</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.094744</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971730</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.050296</td>\n",
       "      <td>0.363990</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.224255</td>\n",
       "      <td>-0.442993</td>\n",
       "      <td>1</td>\n",
       "      <td>1.634198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.196366</td>\n",
       "      <td>-0.976136</td>\n",
       "      <td>-1.049750</td>\n",
       "      <td>-1.092248</td>\n",
       "      <td>-1.238745</td>\n",
       "      <td>-0.860651</td>\n",
       "      <td>-1.071632</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>-1.202899</td>\n",
       "      <td>-1.241514</td>\n",
       "      <td>-1.22241</td>\n",
       "      <td>-1.241936</td>\n",
       "      <td>0.324656</td>\n",
       "      <td>20.150000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.085032</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.286142</td>\n",
       "      <td>-1.238202</td>\n",
       "      <td>-1.175341</td>\n",
       "      <td>-0.118472</td>\n",
       "      <td>-0.292838</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.414467</td>\n",
       "      <td>-1.577770</td>\n",
       "      <td>-1.568050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.511082</td>\n",
       "      <td>-1.651729</td>\n",
       "      <td>-1.637589</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000611</td>\n",
       "      <td>0</td>\n",
       "      <td>0.987992</td>\n",
       "      <td>0.570148</td>\n",
       "      <td>-0.335794</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.183944</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.563369</td>\n",
       "      <td>1</td>\n",
       "      <td>1.123685</td>\n",
       "      <td>1.450854</td>\n",
       "      <td>-0.587622</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933880</td>\n",
       "      <td>1</td>\n",
       "      <td>1.037445</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.128574</td>\n",
       "      <td>0</td>\n",
       "      <td>0.216803</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.062344</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.417092</td>\n",
       "      <td>0</td>\n",
       "      <td>0.579160</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.024149</td>\n",
       "      <td>-0.097513</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.224255</td>\n",
       "      <td>-1.432267</td>\n",
       "      <td>1</td>\n",
       "      <td>1.677407</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.235698</td>\n",
       "      <td>-1.051881</td>\n",
       "      <td>-1.125406</td>\n",
       "      <td>-1.130653</td>\n",
       "      <td>-1.276986</td>\n",
       "      <td>-0.932983</td>\n",
       "      <td>-1.143730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      open      high      low     close    volume    close_1 result_1    perf_1   close_14 result_14   perf_14  results  bb_upper  bb_middle  bb_lower    bb_pct  bb_bandwidth bb_squeeze bb_signalup bb_signaldn  bb_signal     ema50    ema150    ema200  ema_signal1  ema_signal2    kama50   kama150   kama200  kama_signal1  kama_signal2       sar  sar_signal       adx   plus_di  minus_di adx_trend adx_direction  adx_signal  aroon_osc  aroon_signal       cci  cci_signal      macd  \\\n",
       "0 2010-01-04 -1.169181 -1.157299 -1.15216 -1.118360  0.639008  20.680000     True  0.006326  18.719999     False -0.089051        0 -1.321332  -1.279148 -1.221656  1.341702     -0.221542      False        True       False          0 -1.460919 -1.606757 -1.590867            1            1 -1.591087 -1.687069 -1.676153             1             1 -1.257753           1  1.219553  1.264466 -1.068669      True          True           1   1.313898             1  1.004782           1  1.390227   \n",
       "1 2010-01-05 -1.101747 -1.098349 -1.09877 -1.081848  2.142760  20.340000    False -0.016441  18.900000     False -0.086074        0 -1.296796  -1.260871 -1.209938  1.443046     -0.155418      False        True       False          0 -1.444752 -1.598041 -1.584059            1            1 -1.563987 -1.675790 -1.662338             1             1 -1.241792           1  1.295293  1.656456 -1.216766      True          True           1   1.378875             1  1.253799           1  1.457677   \n",
       "2 2010-01-06 -1.070840 -1.106770 -1.15216 -1.177339  1.022456  20.260000    False -0.003933  18.600000     False -0.085546        0 -1.287113  -1.253220 -1.204436  0.550893     -0.134384      False       False       False          0 -1.433052 -1.590782 -1.578353            1            1 -1.545047 -1.666641 -1.653785             1             1 -1.217674           1  1.252893  1.079920 -0.845719      True          True           1   1.313898             1  0.658914           1  1.379371   \n",
       "3 2010-01-07 -1.177611 -1.205021 -1.18307 -1.199808  0.113814  20.110001    False -0.007404  18.570000     False -0.083416        0 -1.284341  -1.244436 -1.189654  0.302612     -0.202750      False       False       False          0 -1.422711 -1.583935 -1.572947            1            1 -1.525456 -1.658651 -1.645185             1             1 -1.195968           1  1.153228  0.830531 -0.626599      True          True           1   1.248921             1  0.087822           1  1.273312   \n",
       "4 2010-01-08 -1.202899 -1.241514 -1.22241 -1.241936  0.324656  20.150000     True  0.001989  18.400000     False -0.085032        0 -1.286142  -1.238202 -1.175341 -0.118472     -0.292838      False       False       False          0 -1.414467 -1.577770 -1.568050            1            1 -1.511082 -1.651729 -1.637589             1             1 -1.000611           0  0.987992  0.570148 -0.335794      True          True           1   1.183944             1 -0.563369           1  1.123685   \n",
       "\n",
       "   macd_sigline  macd_hist  macd_signal       ppo  ppo_signal       mfi  mfi_signal       roc  roc_signal       rsi  rsi_signal   ult_osc  ult_signal     willr  wr_signal    ad_osc  ad_signal  stoch_slowk  stoch_slowd  sslow_signal  stoch_fastk  stoch_fastd  srsi_signal      trix  trix_signal  sr_pivotpts   sr_res1   sr_sup1   sr_res2   sr_sup2   sr_res3   sr_sup3  cv_signal  \n",
       "0      1.549988  -0.096380            0  1.164215           1  1.561731           0  1.442156           0  1.356830           0  0.191720           0  1.334246          0  1.564001          1    -0.269844    -0.148049             0     0.698039    -0.732738            0  1.451422            1    -1.142985 -0.827349 -0.890331 -1.051099 -1.175919 -0.728905 -0.909411          0  \n",
       "1      1.555591   0.078078            1  1.171118           1  1.645223           0  1.297461           0  1.505069           0  0.626544           0  1.120493          0  1.930265          1     0.257237    -0.227046             0     1.139066     0.256536            0  1.519308            1    -1.093350 -0.737175 -0.851602 -0.950513 -1.179561 -0.593715 -0.921427          0  \n",
       "2      1.543035  -0.108883            0  1.160075           1  1.403751           0  0.314070           0  0.703464           0  0.062437           0  0.212042          0  1.387529          1     0.803962     0.284918             0    -0.973098     0.361669            1  1.580734            1    -1.145794 -0.878748 -0.952477 -1.042869 -1.189577 -0.767654 -0.978934          0  \n",
       "3      1.509911  -0.320626            0  1.071905           1  1.155536           0 -0.190918           0  0.532015           0 -0.094744           0 -0.001712          0  0.971730          1    -0.050296     0.363990             0    -1.224255    -0.442993            1  1.634198            1    -1.196366 -0.976136 -1.049750 -1.092248 -1.238745 -0.860651 -1.071632          1  \n",
       "4      1.450854  -0.587622            0  0.933880           1  1.037445           0 -0.128574           0  0.216803           0 -0.062344           0 -0.417092          0  0.579160          1    -1.024149    -0.097513             0    -1.224255    -1.432267            1  1.677407            1    -1.235698 -1.051881 -1.125406 -1.130653 -1.276986 -0.932983 -1.143730          0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dftouse[mask][STANDARDIZABLE])\n",
    "dftouse[STANDARDIZABLE] = scaler.transform(dftouse[STANDARDIZABLE])\n",
    "dftouse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list `lcols` of the columns we will use in our classifier. This list should not contain the response `RESP`. How many features do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "#lcols=list(dftouse.columns)\n",
    "#lcols.remove(u'results')\n",
    "lcols=[]\n",
    "#lcols.append('cv_signal')\n",
    "for c in list(dftouse.columns):\n",
    "    #if c not in INDICATORS and c not in IGNORE:\n",
    "    #if c in INDICATORS:\n",
    "    if c not in IGNORE:  #Original\n",
    "        lcols.append(c)\n",
    "print len(lcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAI9CAYAAAAAWg80AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wEhPxYjhwuAGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107d81ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "featuredf = pd.DataFrame({'feature':lcols})\n",
    "fsX=df[mask][lcols].values\n",
    "fsy=df[mask]['results']\n",
    "\n",
    "# Code/logic from http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "###############################################################################\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "X_indices = np.arange(fsX.shape[-1])\n",
    "\n",
    "###############################################################################\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function: the 15% most significant features\n",
    "selector = SelectPercentile(f_classif, percentile=15)\n",
    "selector.fit(fsX, fsy)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "featuredf['uscore'] = scores\n",
    "scores /= scores.max()\n",
    "featuredf['uscore_scaled'] = scores\n",
    "\n",
    "plt.bar(X_indices - .45, scores, width=.2, label=r'Univariate score ($-Log(p_{value})$)', color='g')\n",
    "\n",
    "###############################################################################\n",
    "# Compare to the weights of an SVM\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(fsX, fsy)\n",
    "\n",
    "svm_weights = (clf.coef_ ** 2).sum(axis=0)\n",
    "featuredf['svm_weight'] = svm_weights\n",
    "svm_weights /= svm_weights.max()\n",
    "featuredf['svm_weight_scaled'] = svm_weights\n",
    "\n",
    "plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight', color='r')\n",
    "\n",
    "clf_selected = svm.SVC(kernel='linear')\n",
    "clf_selected.fit(selector.transform(fsX), fsy)\n",
    "\n",
    "featuredf.sort(['uscore'], ascending=False, inplace=True)\n",
    "\n",
    "svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\n",
    "selfeaturedf = featuredf[:len(svm_weights_selected)].copy()\n",
    "selfeaturedf['svm_weight_sel'] = svm_weights_selected\n",
    "svm_weights_selected /= svm_weights_selected.max()\n",
    "selfeaturedf['svm_weight_sel_scaled'] = svm_weights_selected\n",
    "\n",
    "plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n",
    "        width=.2, label='SVM weights after selection', color='b')\n",
    "\n",
    "\n",
    "plt.title(\"Comparing feature selection\")\n",
    "plt.xlabel('Feature number')\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>uscore</th>\n",
       "      <th>uscore_scaled</th>\n",
       "      <th>svm_weight</th>\n",
       "      <th>svm_weight_scaled</th>\n",
       "      <th>svm_weight_sel</th>\n",
       "      <th>svm_weight_sel_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>stoch_slowk</td>\n",
       "      <td>42.386916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>141466.459074</td>\n",
       "      <td>0.259866</td>\n",
       "      <td>0.446611</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>rsi</td>\n",
       "      <td>40.795512</td>\n",
       "      <td>0.962455</td>\n",
       "      <td>182303.384340</td>\n",
       "      <td>0.334882</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.002191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cci</td>\n",
       "      <td>39.748769</td>\n",
       "      <td>0.937760</td>\n",
       "      <td>70958.466210</td>\n",
       "      <td>0.130347</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.001029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bb_pct</td>\n",
       "      <td>38.135546</td>\n",
       "      <td>0.899701</td>\n",
       "      <td>8.478391</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>plus_di</td>\n",
       "      <td>36.638913</td>\n",
       "      <td>0.864392</td>\n",
       "      <td>125547.445495</td>\n",
       "      <td>0.230624</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.005580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>willr</td>\n",
       "      <td>34.887488</td>\n",
       "      <td>0.823072</td>\n",
       "      <td>6018.478460</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>stoch_fastd</td>\n",
       "      <td>31.419928</td>\n",
       "      <td>0.741265</td>\n",
       "      <td>97341.702359</td>\n",
       "      <td>0.178812</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>stoch_slowd</td>\n",
       "      <td>31.021363</td>\n",
       "      <td>0.731862</td>\n",
       "      <td>153769.282037</td>\n",
       "      <td>0.282466</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.002789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ult_osc</td>\n",
       "      <td>30.493284</td>\n",
       "      <td>0.719403</td>\n",
       "      <td>80728.559706</td>\n",
       "      <td>0.148294</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>minus_di</td>\n",
       "      <td>27.097116</td>\n",
       "      <td>0.639280</td>\n",
       "      <td>76185.827005</td>\n",
       "      <td>0.139949</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>stoch_fastk</td>\n",
       "      <td>26.158318</td>\n",
       "      <td>0.617132</td>\n",
       "      <td>544381.529264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature     uscore  uscore_scaled     svm_weight  svm_weight_scaled  svm_weight_sel  svm_weight_sel_scaled\n",
       "54  stoch_slowk  42.386916       1.000000  141466.459074           0.259866        0.446611               1.000000\n",
       "46          rsi  40.795512       0.962455  182303.384340           0.334882        0.000978               0.002191\n",
       "34          cci  39.748769       0.937760   70958.466210           0.130347        0.000460               0.001029\n",
       "8        bb_pct  38.135546       0.899701       8.478391           0.000016        0.000004               0.000009\n",
       "27      plus_di  36.638913       0.864392  125547.445495           0.230624        0.002492               0.005580\n",
       "50        willr  34.887488       0.823072    6018.478460           0.011056        0.000120               0.000268\n",
       "58  stoch_fastd  31.419928       0.741265   97341.702359           0.178812        0.000081               0.000182\n",
       "55  stoch_slowd  31.021363       0.731862  153769.282037           0.282466        0.001246               0.002789\n",
       "48      ult_osc  30.493284       0.719403   80728.559706           0.148294        0.000003               0.000007\n",
       "28     minus_di  27.097116       0.639280   76185.827005           0.139949        0.000126               0.000282\n",
       "57  stoch_fastk  26.158318       0.617132  544381.529264           1.000000        0.000179               0.000400"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfeaturedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lcols = list(selfeaturedf['feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We create a variable `ccols` which contains all variables not in our indicators list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stoch_slowk',\n",
       " 'rsi',\n",
       " 'cci',\n",
       " 'bb_pct',\n",
       " 'plus_di',\n",
       " 'willr',\n",
       " 'stoch_fastd',\n",
       " 'stoch_slowd',\n",
       " 'ult_osc',\n",
       " 'minus_di',\n",
       " 'stoch_fastk']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccols=[]\n",
    "for c in lcols:\n",
    "    if c not in INDICATORS and c not in IGNORE:\n",
    "        ccols.append(c)\n",
    "print len(ccols), len(INDICATORS)\n",
    "ccols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Train a SVM on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "cv_optimize\n",
    "\n",
    "Inputs\n",
    "------\n",
    "clf : an instance of a scikit-learn classifier\n",
    "parameters: a parameter grid dictionary thats passed to GridSearchCV (see above)\n",
    "X: a samples-features matrix in the scikit-learn style\n",
    "y: the response vectors of 1s and 0s (+ives and -ives)\n",
    "n_folds: the number of cross-validation folds (default 5)\n",
    "score_func: a score function we might want to pass (default python None)\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "The best estimator from the GridSearchCV, after the GridSearchCV has been used to\n",
    "fit the model.\n",
    "     \n",
    "Notes\n",
    "-----\n",
    "see do_classify and the code below for an example of how this is used\n",
    "\"\"\"\n",
    "#your code here\n",
    "def cv_optimize(clf, parameters, X, y, n_folds, score_func):\n",
    "    fitmodel = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=score_func)\n",
    "    fitmodel.fit(X, y)\n",
    "    return fitmodel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    if mask !=None:\n",
    "        print \"using mask\"\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split !=None:\n",
    "        print \"using reuse split\"\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    print confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print \"########################################################\"\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.70\n",
      "Accuracy on test data:     0.70\n",
      "[[89 36]\n",
      " [32 72]]\n",
      "########################################################\n",
      "CPU times: user 250 ms, sys: 2.24 ms, total: 252 ms\n",
      "Wall time: 270 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ela/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clfsvm, Xtrain, ytrain, Xtest, ytest = do_classify(LinearSVC(loss=\"hinge\"), {\"C\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}, dftouse, lcols, u'results',1, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds={}\n",
    "preds_ROI_long={}\n",
    "preds_ROI_longshort={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI baseline: 1.46%\n",
      "ROI \"result\" buy-only: 68.01%\n",
      "ROI \"result\" buy-sell: 173.41%\n",
      "ROI \"result\" buy-only: 166.25%\n",
      "ROI \"result\" buy-sell: 575.63%\n"
     ]
    }
   ],
   "source": [
    "df_pred = df[~mask].reset_index(drop=True)\n",
    "df_pred['baseline'] = np.ones(df_pred.shape[0])\n",
    "\n",
    "balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'baseline', 'close', False, [1])\n",
    "print \"ROI baseline: {0:.2f}%\".format(ROI*100)\n",
    "preds[\"baseline_long\"] = df_pred['baseline']\n",
    "preds_ROI_long[\"baseline_long\"] = ROI\n",
    "preds_ROI_longshort[\"baseline_long\"] = ROI\n",
    "\n",
    "balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'results', 'close', False, [1])\n",
    "print 'ROI \"result\" buy-only: {0:.2f}%'.format(ROI*100)\n",
    "preds[\"baseline_ema\"] = df_pred['results']\n",
    "preds_ROI_long[\"baseline_ema\"] = ROI\n",
    "balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'results', 'close', False, [0,1])\n",
    "print 'ROI \"result\" buy-sell: {0:.2f}%'.format(ROI*100)\n",
    "preds_ROI_longshort[\"baseline_ema\"] = ROI\n",
    "\n",
    "balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'result_1', 'close', False, [1])\n",
    "print 'ROI \"result\" buy-only: {0:.2f}%'.format(ROI*100)\n",
    "preds[\"baseline_max\"] = df_pred['result_1']\n",
    "preds_ROI_long[\"baseline_max\"] = ROI\n",
    "balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'result_1', 'close', False, [0,1])\n",
    "print 'ROI \"result\" buy-sell: {0:.2f}%'.format(ROI*100)\n",
    "preds_ROI_longshort[\"baseline_max\"] = ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(clf, desc):\n",
    "\n",
    "    ypred = clf.predict(Xtest)\n",
    "    df_pred = df[~mask].reset_index(drop=True)\n",
    "    df_pred['pred_result'] = ypred\n",
    "    df_pred['result_baseline'] = np.ones(df_pred.shape[0])\n",
    "    print \"accuracy on test set: {0:.3f}\".format((df_pred.results == df_pred.pred_result).sum()/float(len(df_pred)))\n",
    "    \n",
    "    balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'pred_result', 'close', False, [1])\n",
    "    print 'ROI \"pred\" buy-only: {0:.2f}%'.format(ROI*100)\n",
    "    preds[desc] = ypred\n",
    "    preds_ROI_long[desc] = ROI\n",
    "\n",
    "    balance, profit, ROI, balovertime, signals = evaluate_profit(df_pred, startdate, enddate, 10000, 'pred_result', 'close', False, [0,1])\n",
    "    print 'ROI \"pred\" buy-sell: {0:.2f}%'.format(ROI*100)\n",
    "    preds_ROI_longshort[desc] = ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.703\n",
      "ROI \"pred\" buy-only: 4.79%\n",
      "ROI \"pred\" buy-sell: 7.67%\n"
     ]
    }
   ],
   "source": [
    "evaluate(clfsvm, \"svm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained ought to be very similar to the efforts you put in earlier. If not its likely you wrote `cv_optimize` wrong. (Remember that we are using the same mask).\n",
    "\n",
    "We'll reuse the training and test sets you computed above later in the homework. We do this by putting them into a dictionary `reuse_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_split=dict(Xtrain=Xtrain, Xtest=Xtest, ytrain=ytrain, ytest=ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimate costs and benefits from assumptions and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data is highly asymmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First notice that our data set is very highly asymmetric, with positive `RESP`onses only making up 16-17% of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole data set 0.465366509751\n",
      "training set 0.467408585056 test set 0.454148471616\n"
     ]
    }
   ],
   "source": [
    "print \"whole data set\", dftouse['results'].mean()#Highly asymmetric\n",
    "print \"training set\", dftouse['results'][mask].mean(), \"test set\", dftouse['results'][~mask].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that a classifier which predicts that EVERY customer is a negative has an accuracy rate of 83-84%. By this we mean that **a classifier that predicts that no customer will respond to our mailing** has an accuracy of 83-84%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.69\n",
      "Accuracy on test data:     0.68\n",
      "[[81 44]\n",
      " [30 74]]\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ela/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clflog = LogisticRegression(penalty=\"l1\")\n",
    "parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "clflog, Xtrain, ytrain, Xtest, ytest = do_classify(clflog, parameters, dftouse, lcols, u'results',1, mask=mask, reuse_split=reuse_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clflog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.677\n",
      "ROI \"pred\" buy-only: 7.70%\n",
      "ROI \"pred\" buy-sell: 13.73%\n"
     ]
    }
   ],
   "source": [
    "evaluate(clflog, \"logistic_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.98\n",
      "Accuracy on test data:     0.65\n",
      "[[84 41]\n",
      " [39 65]]\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ela/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clfraf = RandomForestClassifier(n_jobs=2)\n",
    "parameters = {}#{\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "clfraf, Xtrain, ytrain, Xtest, ytest = do_classify(clfraf, parameters, dftouse, lcols, u'results',1, mask=mask, reuse_split=reuse_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.651\n",
      "ROI \"pred\" buy-only: 7.99%\n",
      "ROI \"pred\" buy-sell: 14.11%\n"
     ]
    }
   ],
   "source": [
    "evaluate(clfraf, \"random_forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trying to improve the SVM: Feature Selection and Data Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did the previous section right, you will find that the logistic regression model provides a better profit over some section of the profit curve than the baseline \"send to everyone\" classifier, while the SVM classifier is generally poor. At this might we might want to try all kinds of classifiers: from perceptrons to random forests. In the interest of time, and to study the SVM in some more detail, we'll restrict ourselves to trying to improve the SVM performance here. In real life you would try other classifiers as well.\n",
    "\n",
    " We wont be exhaustive in this improvement process either(which is something you should do on your project) in the interests of time, but we'll explore if feature-selection on the  SVM, and data balancing on the SVM (SVM's are known to perform better on balanced data) help.\n",
    " \n",
    "( An aside: many classifiers such as SVM and decision trees struggle in their techniques on imbalanced data. You can read more at: see Weiss, Gary M., and Foster Provost. \"The effect of class distribution on classifier learning: an empirical study.\" Rutgers Univ (2001). Also see http://pages.stern.nyu.edu/~fprovost/Papers/skew.PDF and http://www.cs.ox.ac.uk/people/vasile.palade/papers/Class-Imbalance-SVM.pdf for multiple ways to deal with the imbalance problem: balancing is not always the best option. `Sklearn` also provides a class weighting strategy: http://scikit-learn.org/stable/modules/svm.html#unbalanced-problems ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso, for example, implements internally, a form of feature selection by setting many coefficients to zero. Let us find coefficients that are non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non zero lasso features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function `nonzero_lasso` which takes the fit classifier `clfloglasso` as an argument, and spits out a dataframe of coefficients, sorted by the absolute magnitude of the coefficients. This way we can see which features dominated the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nonzero_lasso(clf):\n",
    "    featuremask=(clf.coef_ !=0.0)[0]\n",
    "    return pd.DataFrame(dict(feature=lcols, coef=clf.coef_[0], abscoef=np.abs(clf.coef_[0])))[featuremask].sort('abscoef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abscoef</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>plus_di</th>\n",
       "      <td>0.402742</td>\n",
       "      <td>-0.402742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_slowk</th>\n",
       "      <td>0.336490</td>\n",
       "      <td>-0.336490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_fastk</th>\n",
       "      <td>0.269712</td>\n",
       "      <td>-0.269712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsi</th>\n",
       "      <td>0.142509</td>\n",
       "      <td>-0.142509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              abscoef      coef\n",
       "feature                        \n",
       "plus_di      0.402742 -0.402742\n",
       "stoch_slowk  0.336490 -0.336490\n",
       "stoch_fastk  0.269712 -0.269712\n",
       "rsi          0.142509 -0.142509"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_importances=nonzero_lasso(clflog)\n",
    "lasso_importances.set_index(\"feature\", inplace=True)\n",
    "lasso_importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Feature importance using correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a notion of which features are important in the classification process by seeing how they correlate with the response. Implement some code to obtain the Pearson correlation coefficient between each of our features and the response. Do this on the training set only! Create a dataframe indexed by the features, which has columns `abscorr` the absolute value of the correlation and `corr` the value of the correlation. Sort the dataframe by `abscorr`, highest first, and show the top 25 features with the highest absolute correlation. Is there much overlap with the feature selection performed by the LASSO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abscorr</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stoch_slowk</th>\n",
       "      <td>0.374288</td>\n",
       "      <td>-0.374288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsi</th>\n",
       "      <td>0.367538</td>\n",
       "      <td>-0.367538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cci</th>\n",
       "      <td>0.363008</td>\n",
       "      <td>-0.363008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bb_pct</th>\n",
       "      <td>0.355882</td>\n",
       "      <td>-0.355882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plus_di</th>\n",
       "      <td>0.349103</td>\n",
       "      <td>-0.349103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willr</th>\n",
       "      <td>0.340952</td>\n",
       "      <td>-0.340952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_fastd</th>\n",
       "      <td>0.324055</td>\n",
       "      <td>-0.324055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_slowd</th>\n",
       "      <td>0.322043</td>\n",
       "      <td>-0.322043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ult_osc</th>\n",
       "      <td>0.319353</td>\n",
       "      <td>-0.319353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minus_di</th>\n",
       "      <td>0.301359</td>\n",
       "      <td>0.301359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_fastk</th>\n",
       "      <td>0.296154</td>\n",
       "      <td>-0.296154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              abscorr      corr\n",
       "feature                        \n",
       "stoch_slowk  0.374288 -0.374288\n",
       "rsi          0.367538 -0.367538\n",
       "cci          0.363008 -0.363008\n",
       "bb_pct       0.355882 -0.355882\n",
       "plus_di      0.349103 -0.349103\n",
       "willr        0.340952 -0.340952\n",
       "stoch_fastd  0.324055 -0.324055\n",
       "stoch_slowd  0.322043 -0.322043\n",
       "ult_osc      0.319353 -0.319353\n",
       "minus_di     0.301359  0.301359\n",
       "stoch_fastk  0.296154 -0.296154"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "correlations=[]\n",
    "dftousetrain=dftouse[mask]\n",
    "for col in lcols:\n",
    "    r=pearsonr(dftousetrain[col], dftousetrain['results'])[0]\n",
    "    correlations.append(dict(feature=col,corr=r, abscorr=np.abs(r)))\n",
    "\n",
    "bpdf=pd.DataFrame(correlations).sort('abscorr', ascending=False)\n",
    "bpdf.set_index(['feature'], inplace=True)\n",
    "bpdf.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Feature Select?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons feature selection is done, automatically or otherwise, is that there might be strong correlations between features. Also recall polynomial regression: a large number of features can lead to overfitting. Feature selection helps curb the problem of the curse of dimensionality, where centrality measures often used in statistics go wonky at higher dimensions. Between feature-engineering which we did some of, earlier, and feature selection, is where a lot of smarts and domain knowledge comes in. You will gain this with experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline to feature-select, standardize and train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use sklearn pipelines to do correlation-with-response based feature selection for our SVM model. Maybe such feature-selection will improve the abysmal performance. \n",
    "\n",
    "This does not reduce the collinearity amongst the features, for which one either needs PCA, ICA, or some feature selection using the forward-backward algorithm. We do not have the time to approach it here. \n",
    "\n",
    "Its very important to do response based feature selection in the right way. If you remember, we separately standardized the training and test sets. This was to prevent **any** information about the overall mean and standard deviation leaking into the test set. \n",
    "\n",
    "But we played a bit loose with the rules there. We standardized on the entire training set. Instead we should have been standardizing separately in each cross-validation fold. There the original training set would be broken up into a sub-training and validation set, the standardization needed to be done on those separately. This can be implemented with `sklearn` pipelines.\n",
    "\n",
    "Such kind of \"data snooping\" is relatively benign though, as it used no information about the response variable. But if you do any feature selection which uses the response variable, such as choosing the \"k\" most correlated variables from above, its not benign any more. This is because you have leaked the response from the validation into your sub-training set, and cannot thus be confident about your predictions: you might overfit. In such a situation, you must do the feature selection inside the cross-validation fold. See http://nbviewer.ipython.org/github/cs109/content/blob/master/lec_10_cross_val.ipynb from the 2013 course for a particularly dastardly case of this, where you see that the problem is particularly exacerbated when you have many more features than samples.\n",
    "\n",
    "Lets do this here using sklearn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a scorer which returns the absolute values of the pearson correlation between the feature and the response for each sample. The specific form of the scorer is dictated to us in the API docs for `SelectKBest`, see [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html): the first argument must be an array of scores, and the second an array of p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson_scorer(X,y):\n",
    "    rs=np.zeros(X.shape[1])\n",
    "    pvals=np.zeros(X.shape[1])\n",
    "    i=0\n",
    "    for v in X.T:\n",
    "        rs[i], pvals[i]=pearsonr(v, y)\n",
    "        i=i+1\n",
    "    return np.abs(rs), pvals    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply the feature selection to a model which did not have any automatic feature selection and performed rather poorly before: the linear SVM. \n",
    "\n",
    "The `Pipeline` feature of sklearn chains various parts of a machine learning algorithm together. In this case we want to chain feature-selection and training in such a way that both happen freshly for each cross-validation fold (we wont bother to standardize in each cross-validation fold separately here for brevity, although you might want to do this).\n",
    "We use the `SelectKBest` meta estimator to select the 25 most correlated/anti-correlated features. We create an instance of this meta-estimator, `selectorlinearsvm`. We then combine it with the linear SVC estimators into the pipeline `pipelinearsvm`: the `Pipeline` function simply takes a list of `scikit-learn` estimators and wraps them together into a new estimator object, which can then be passed to `GridSearchCV` via our `do_classify` function. Notice how this new estimator object can be used exactly the same way as a single classifier can be used in `scikit-learn`..this uniformity of interface is one of the nice features of `sklearn`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectorlinearsvm = SelectKBest(k='all', score_func=pearson_scorer)\n",
    "pipelinearsvm = Pipeline([('select', selectorlinearsvm), ('svm', LinearSVC(loss=\"hinge\"))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us run the pipelined classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the classifier and compare the results using the ROC curve to the previous SVM result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.70\n",
      "Accuracy on test data:     0.70\n",
      "[[89 36]\n",
      " [32 72]]\n",
      "########################################################\n"
     ]
    }
   ],
   "source": [
    "pipelinearsvm, _,_,_,_  = do_classify(pipelinearsvm, {\"svm__C\": [0.00001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, dftouse,lcols, u'results',1, reuse_split=reuse_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What features did the pipelined classifier use? We can access them so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stoch_slowk', 'rsi', 'cci', 'bb_pct', 'plus_di', 'willr',\n",
       "       'stoch_fastd', 'stoch_slowd', 'ult_osc', 'minus_di', 'stoch_fastk'], \n",
       "      dtype='|S11')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(lcols)[pipelinearsvm.get_params()['select'].get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the ROC curves, using the label `svm-feature-selected` for the pipelined classifier `pipelinearsvm`. We plot it alongside the older logistic-with lasso and all-features SVM for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.703\n",
      "ROI \"pred\" buy-only: 4.79%\n",
      "ROI \"pred\" buy-sell: 7.67%\n"
     ]
    }
   ],
   "source": [
    "evaluate(pipelinearsvm, \"pipelinearsvm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing train set to test set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588 670\n"
     ]
    }
   ],
   "source": [
    "jtrain=np.arange(0, ytrain.shape[0])\n",
    "n_pos=len(jtrain[ytrain==1])\n",
    "n_neg=len(jtrain[ytrain==0])\n",
    "print n_pos, n_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more negative samples in the training set. We want to balance the negative samples to the positive samples. So lets sample $n_{+}$ samples from the negative samples in the training set (without replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ineg = np.random.choice(jtrain[ytrain==0], 500, replace=False)#n_pos, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate all the indexes and use them to select a new training set from the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1088,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alli=np.concatenate((jtrain[ytrain==1], ineg))\n",
    "alli.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1088, 11), (1088,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_new = Xtrain[alli]\n",
    "ytrain_new = ytrain[alli]\n",
    "Xtrain_new.shape, ytrain_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store these into a new split variable `reuse_split_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reuse_split_new=dict(Xtrain=Xtrain_new, Xtest=Xtest, ytrain=ytrain_new, ytest=ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test sets are identical as before. This is as, even though we are training the SVM classifier in the \"naturally\" unfound situation of balanced classes, we **must test it in the real-world scenario of imbalance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Train a linear SVM on this balanced set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a non-feature-selected linear SVM on this new balanced set as a comparison to both our old SVM on the imbalanced data set `clfsvm` and the feature-selected linear SVM `pipelinearsvm`. Store this new classifier in the variable `clfsvm_b`.\n",
    "\n",
    "Compare the performances of all three of these classifiers using the roc curve plot, with the new `clfsvm_b` labeled as `svm-all-features-balanced`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.69\n",
      "Accuracy on test data:     0.68\n",
      "[[75 50]\n",
      " [24 80]]\n",
      "########################################################\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "clfsvm_b = Pipeline([('select', selectorlinearsvm), ('svm', LinearSVC(loss=\"hinge\"))])\n",
    "clfsvm_b, _,_,_,_  = do_classify(clfsvm_b, {\"svm__C\": [0.00001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('select', SelectKBest(k='all', score_func=<function pearson_scorer at 0x109ef5398>)), ('svm', LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfsvm_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.677\n",
      "ROI \"pred\" buy-only: 9.60%\n",
      "ROI \"pred\" buy-sell: 17.78%\n"
     ]
    }
   ],
   "source": [
    "evaluate(clfsvm_b, \"svm_balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Implement a RBF based pipelined (feature-selected) classifier on the balanced set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.68\n",
      "Accuracy on test data:     0.69\n",
      "[[84 41]\n",
      " [29 75]]\n",
      "########################################################\n",
      "CPU times: user 1.95 s, sys: 29.2 ms, total: 1.98 s\n",
      "Wall time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selectorsvm2 = SelectKBest(k='all', score_func=pearson_scorer)\n",
    "pipesvm2 = Pipeline([('select2', selectorsvm2), ('svm2', SVC())])\n",
    "pipesvm2, _,_,_,_  = do_classify(pipesvm2, {\"svm2__C\": [1e8, 1e9, 1e10], \"svm2__gamma\": [ 1e-9, 1e-10, 1e-11]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'select2': SelectKBest(k='all', score_func=<function pearson_scorer at 0x109ef5398>),\n",
       " 'select2__k': 'all',\n",
       " 'select2__score_func': <function __main__.pearson_scorer>,\n",
       " 'svm2': SVC(C=1000000000.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
       "   gamma=1e-10, kernel='rbf', max_iter=-1, probability=False,\n",
       "   random_state=None, shrinking=True, tol=0.001, verbose=False),\n",
       " 'svm2__C': 1000000000.0,\n",
       " 'svm2__cache_size': 200,\n",
       " 'svm2__class_weight': None,\n",
       " 'svm2__coef0': 0.0,\n",
       " 'svm2__degree': 3,\n",
       " 'svm2__gamma': 1e-10,\n",
       " 'svm2__kernel': 'rbf',\n",
       " 'svm2__max_iter': -1,\n",
       " 'svm2__probability': False,\n",
       " 'svm2__random_state': None,\n",
       " 'svm2__shrinking': True,\n",
       " 'svm2__tol': 0.001,\n",
       " 'svm2__verbose': False}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipesvm2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.694\n",
      "ROI \"pred\" buy-only: 16.54%\n",
      "ROI \"pred\" buy-sell: 33.20%\n"
     ]
    }
   ],
   "source": [
    "evaluate(pipesvm2, \"pipesvm2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yvan ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############====================== Log Regression =====================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.69\n",
      "Accuracy on test data:     0.67\n",
      "[[74 51]\n",
      " [24 80]]\n",
      "########################################################\n",
      "accuracy on test set: 0.672\n",
      "ROI \"pred\" buy-only: 8.68%\n",
      "ROI \"pred\" buy-sell: 15.86%\n",
      "#############====================== Linear SVM ========================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.69\n",
      "Accuracy on test data:     0.68\n",
      "[[75 50]\n",
      " [24 80]]\n",
      "########################################################\n",
      "accuracy on test set: 0.677\n",
      "ROI \"pred\" buy-only: 9.60%\n",
      "ROI \"pred\" buy-sell: 17.78%\n",
      "#############====================== RBF SVC ===========================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.68\n",
      "Accuracy on test data:     0.69\n",
      "[[84 41]\n",
      " [29 75]]\n",
      "########################################################\n",
      "accuracy on test set: 0.694\n",
      "ROI \"pred\" buy-only: 16.54%\n",
      "ROI \"pred\" buy-sell: 33.20%\n",
      "#############====================== Random Forest =====================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.96\n",
      "Accuracy on test data:     0.62\n",
      "[[64 61]\n",
      " [26 78]]\n",
      "########################################################\n",
      "accuracy on test set: 0.620\n",
      "ROI \"pred\" buy-only: 0.52%\n",
      "ROI \"pred\" buy-sell: -0.80%\n",
      "#############====================== Extra Trees= =====================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.78\n",
      "Accuracy on test data:     0.62\n",
      "[[65 60]\n",
      " [26 78]]\n",
      "########################################################\n",
      "accuracy on test set: 0.624\n",
      "ROI \"pred\" buy-only: 9.33%\n",
      "ROI \"pred\" buy-sell: 17.26%\n",
      "#############====================== AdaBoost ==========================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.71\n",
      "Accuracy on test data:     0.63\n",
      "[[64 61]\n",
      " [23 81]]\n",
      "########################################################\n",
      "accuracy on test set: 0.633\n",
      "ROI \"pred\" buy-only: 3.55%\n",
      "ROI \"pred\" buy-sell: 5.26%\n",
      "#############====================== Gaussian NB ==========================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.68\n",
      "Accuracy on test data:     0.69\n",
      "[[81 44]\n",
      " [27 77]]\n",
      "########################################################\n",
      "accuracy on test set: 0.690\n",
      "ROI \"pred\" buy-only: 10.51%\n",
      "ROI \"pred\" buy-sell: 19.91%\n",
      "#############====================== Gradient Boosting ====================#############\n",
      "using reuse split\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.73\n",
      "Accuracy on test data:     0.63\n",
      "[[67 58]\n",
      " [26 78]]\n",
      "########################################################\n",
      "accuracy on test set: 0.633\n",
      "ROI \"pred\" buy-only: 6.03%\n",
      "ROI \"pred\" buy-sell: 10.21%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# selector of  features\n",
    "featSelector = SelectKBest(k='all', score_func=pearson_scorer)\n",
    "\n",
    "print \"#############====================== Log Regression =====================#############\"\n",
    "pipeLR = Pipeline([('select', featSelector), ('LR', LogisticRegression(penalty=\"l1\"))])\n",
    "pipeLR, _,_,_,_  = do_classify(pipeLR, {\"LR__C\": [0.005, 0.01, 0.02, 0.05, 10.0]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(pipeLR, \"pipe_Log_Regr\")\n",
    "print \"#############====================== Linear SVM ========================#############\"\n",
    "clfsvm_b = Pipeline([('select', featSelector), ('svm', LinearSVC(loss=\"hinge\"))])\n",
    "clfsvm_b, _,_,_,_  = do_classify(clfsvm_b, {\"svm__C\": [0.00001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(clfsvm_b, \"linear_svm\")\n",
    "print \"#############====================== RBF SVC ===========================#############\"\n",
    "pipesvm2 = Pipeline([('select', featSelector), ('svm2', SVC())])\n",
    "pipesvm2, _,_,_,_  = do_classify(pipesvm2, {\"svm2__C\": [1e8, 1e9, 1e10], \"svm2__gamma\": [ 1e-9, 1e-10, 1e-11]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(pipesvm2, \"rbf_svc\")\n",
    "print \"#############====================== Random Forest =====================#############\"\n",
    "pipeRF = Pipeline([('select', featSelector), ('RF', RandomForestClassifier())])\n",
    "pipeRF, _,_,_,_  = do_classify(pipeRF, {\"RF__max_depth\": [3,5,7,10,15,25,50], \"RF__n_estimators\": [5,10,20,40],\"RF__max_features\": [1,2,3]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(pipeRF, \"rand_forest\")\n",
    "print \"#############====================== Extra Trees= =====================#############\"\n",
    "pipeET = Pipeline([('select', featSelector), ('ET', RandomForestClassifier())])\n",
    "pipeET, _,_,_,_  = do_classify(pipeET, {\"ET__max_depth\": [3,5,7,10,15,25,50], \"ET__n_estimators\": [5,10,20,40],\"ET__max_features\": [1,2,3]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(pipeET, \"extra_trees\")\n",
    "print \"#############====================== AdaBoost ==========================#############\"\n",
    "pipeAda = Pipeline([('select', featSelector), ('Ada', AdaBoostClassifier())])\n",
    "pipeAda, _,_,_,_  = do_classify(pipeAda, {\"Ada__n_estimators\": [5,10,20,40],\"Ada__learning_rate\": [0.1,0.5,1.0]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(pipeAda, \"ada_boost\")\n",
    "print \"#############====================== Gaussian NB ==========================#############\"\n",
    "pipeNB = Pipeline([('select', featSelector), ('NB', GaussianNB())])\n",
    "pipeNB, _,_,_,_  = do_classify(pipeNB, {}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(pipeNB, \"gaussian_nb\")\n",
    "print \"#############====================== Gradient Boosting ====================#############\"\n",
    "pipeGB = Pipeline([('select', featSelector), ('GB', GradientBoostingClassifier())])\n",
    "pipeGB, _,_,_,_  = do_classify(pipeGB, {\"GB__n_estimators\": [5,10,20,40],\"GB__learning_rate\": [0.1,0.5,1.0]}, dftouse,lcols, u'results',1, reuse_split=reuse_split_new)\n",
    "evaluate(pipeGB, \"gradient_boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm',\n",
       " 'pipelinearsvm',\n",
       " 'rbf_svc',\n",
       " 'linear_svm',\n",
       " 'ada_boost',\n",
       " 'rand_forest',\n",
       " 'pipesvm2',\n",
       " 'extra_trees',\n",
       " 'logistic_regression',\n",
       " 'baseline_ema',\n",
       " 'pipe_Log_Regr',\n",
       " 'random_forest',\n",
       " 'gaussian_nb',\n",
       " 'svm_balanced',\n",
       " 'gradient_boosting',\n",
       " 'baseline_max',\n",
       " 'baseline_long']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada_boost': 0.035471027999999571,\n",
       " 'baseline_ema': 0.68012476040000114,\n",
       " 'baseline_long': 0.014620033999999942,\n",
       " 'baseline_max': 1.6624857745999984,\n",
       " 'extra_trees': 0.093302002099999656,\n",
       " 'gaussian_nb': 0.10505078660000036,\n",
       " 'gradient_boosting': 0.060252845299999719,\n",
       " 'linear_svm': 0.095970888800000473,\n",
       " 'logistic_regression': 0.07702581840000039,\n",
       " 'pipe_Log_Regr': 0.086752889900000088,\n",
       " 'pipelinearsvm': 0.047863849500000284,\n",
       " 'pipesvm2': 0.16541089769999998,\n",
       " 'rand_forest': 0.0052088224000000991,\n",
       " 'random_forest': 0.079922711199999691,\n",
       " 'rbf_svc': 0.16541089769999998,\n",
       " 'svm': 0.047863849500000284,\n",
       " 'svm_balanced': 0.095970888800000473}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ROI_long"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Without UFS:\n",
    "{'ada_boost': 0.053562786300000519,\n",
    " 'baseline_ema': 0.68012476040000114,\n",
    " 'baseline_long': 0.014620033999999942,\n",
    " 'baseline_max': 1.6624857745999984,\n",
    " 'extra_trees': 0.056811064999999508,\n",
    " 'gaussian_nb': 0.13998074890000006,\n",
    " 'gradient_boosting': 0.07254088620000021,\n",
    " 'linear_svm': 0.074455847799999544,\n",
    " 'logistic_regression': 0.070032704300000664,\n",
    " 'pipe_Log_Regr': 0.095388889299999932,\n",
    " 'pipelinearsvm': 0.028158891500000026,\n",
    " 'pipesvm2': 0.095877789599999777,\n",
    " 'rand_forest': 0.047138921100000154,\n",
    " 'random_forest': 0.095902069699999815,\n",
    " 'rbf_svc': 0.10827675600000039,\n",
    " 'svm': 0.13895765499999999,\n",
    " 'svm_balanced': 0.067999918099999737}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
